{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently my team was tasked for lifting and shifting a model currently deployed in Local linux machine to Google Cloud Platform.I found this process quite challenging since storage buckets and IAM permissions on Google cloud platform are controlled by external vendor and since it was first live implementation it was important to understand the architecture of data streams set up\n",
    "\n",
    "\n",
    "The process of deploying and serving model predictions from GCP is quite straightforward, given the set up is right in first place. ( More on that later) \n",
    "\n",
    "There are broadly speaking 5 main steps to deploy model \n",
    "\n",
    "1. Have a packaged model ready for deployement \n",
    "For our case we used a model trained offline using sklearn and Regularised logistic regression. I will be using sample code with breast cancer data (available in sklearn dataset package) to show to package your model correctly \n",
    "\n",
    "2. Create a Google Cloud Storage Bucket\n",
    "Pitfall : Create the GCS bucket as regional and in the similar region to your Python Notebook instance otherwise your  packaged model packaged file would not be accessible and you may encounter an error \"Bad Model Detected\"\n",
    "\n",
    "3. Upload your Packaged Model to GCS Bucket created in above step \n",
    "\n",
    "4. Define an AI platform prediction resource \n",
    "\n",
    "5. Define an AI platform version resource \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging a model correctly \n",
    "\n",
    "The below sample codes shows how to create a baseline model using breast cancer data and save the model to be copied to GCS bucket and used for creating version \n",
    "\n",
    "Constraints \n",
    "\n",
    "1. GCP accepts models in format joblib,pickle or protobuf formats\n",
    "2. GCP has a standard nomenclature for accepting model names and ideally all trained models should be named as model.pkl or model.joblib , naming models with a different name would throw error during version create time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport sklearn\\nfrom sklearn import datasets \\nfrom sklearn import linear_model\\nimport pickle \\n\\n#load data\\nbreast_cancer = datasets.load_breast_cancer()\\n\\n#define baseline model\\nmodel = sklearn.linear_model.LogisticRegression()\\n\\n#save baseline model \\nmodel.fit(breast_cancer.data, breast_cancer.target) \\nwith open('model.pkl', 'wb') as model_file: \\n    pickle.dump(model, model_file)\\n    \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import sklearn\n",
    "from sklearn import datasets \n",
    "from sklearn import linear_model\n",
    "import pickle \n",
    "\n",
    "#load data\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "#define baseline model\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "#save baseline model \n",
    "model.fit(breast_cancer.data, breast_cancer.target) \n",
    "with open('model.pkl', 'wb') as model_file: \n",
    "    pickle.dump(model, model_file)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Google Cloud Storage Bucket \n",
    "\n",
    "A storage bucket is needed to act as a placeholder for your offline trained model to be deployed. Creating bucket can be done using command line interface or using GUI and following the steps attached\n",
    "\n",
    "As mentioned earlier it is very important to make sure the region name is same for GCS bucket to that of Notebook instance,This would allow for error free deployement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$gsutil mb -l <region_name> gs://<bucket_name>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "$gsutil mb -l <region_name> gs://<bucket_name>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![](/images/GCP/Model_Deployment/Storage_Bucket_Creation_1.png){: .center-image }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload your Packaged Model to GCS Bucket created in above step \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![](/images/GCP/Model_Deployment/Upload_Model_GCS_Bucket.png){: .center-image }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an AI platform prediction resource \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some pre-requistes before a prediction resouce can be defined. If there is no Python/Tensorflow instance ( Depending on the model you are trying to deploy on cloud) an instance needs to be created \n",
    "\n",
    "The model API is by deafult enabled if not kindly enable the API manually in your case \n",
    "\n",
    "Once done create a model resource to act as place holder for when the version is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![](/images/GCP/Model_Deployment/Create_Python_Instance.png){: .center-image }\n",
    "![](/images/GCP/Model_Deployment/enable_models_api.png){: .center-image }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an AI platform version resource \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the above steps have been warning/error free then the below should run smoothly. However some pre-requiste information is needed before creating the version and the commands below can help in identifying the runtime and framework version for the model created\n",
    "\n",
    "It is very important to specify the right versions for compiling the model else it would throw errors\n",
    "\n",
    "Once the job is submitted for version creation it would be completed in 2-3 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "# Verify python version installed\n",
    "from platform import python_version\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![](/images/GCP/Model_Deployment/model_version.png){: .center-image }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model all Deployed and Ready for serving predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "![](/images/GCP/Model_Deployment/model_version_created.png){: .center-image }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix \n",
    "\n",
    "Offical Documentation for deploying models on cloud platform \n",
    "https://cloud.google.com/ai-platform/prediction/docs/deploying-models\n",
    "    \n",
    "Offical Documentation for serving predictions using deployed models\n",
    "https://cloud.google.com/ai-platform/prediction/docs/online-predict\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
